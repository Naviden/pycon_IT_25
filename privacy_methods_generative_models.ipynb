{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2bc776e-4ea2-4bfd-a122-695d807d43fd",
   "metadata": {},
   "source": [
    "# Privacy Techniques for Generative Models\n",
    "\n",
    "## 1. Differential Privacy with Opacus\n",
    "\n",
    "Differential Privacy (DP) is a formal framework that ensures the inclusion or exclusion of a single training data point does not significantly affect the output of a model. In this example, we use the `Opacus` library with PyTorch to train a model using DP-SGD (Stochastic Gradient Descent with noise added to gradients), which helps protect individual data samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4b336c-e0e7-4178-add4-305950acac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Loss = 2.3245\n",
      "Batch 100: Loss = 2.2106\n",
      "Batch 200: Loss = 2.0593\n",
      "Batch 300: Loss = 1.9786\n"
     ]
    }
   ],
   "source": [
    "from opacus import PrivacyEngine\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Use CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define model\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(28 * 28, 10)\n",
    ").to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Data\n",
    "train_loader = DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=64,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Attach Privacy Engine with required max_grad_norm\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, train_loader = privacy_engine.make_private_with_epsilon(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    target_epsilon=10,\n",
    "    target_delta=1e-5,\n",
    "    epochs=1,\n",
    "    max_grad_norm=1.0\n",
    ")\n",
    "\n",
    "# Training loop with visible output\n",
    "for batch_idx, (x, y) in enumerate(train_loader):\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(model(x), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch_idx % 100 == 0:\n",
    "        print(f\"Batch {batch_idx}: Loss = {loss.item():.4f}\")\n",
    "        \n",
    "    if batch_idx >= 300:  # Limit for quick demo\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f283b0-ce23-4c64-8ede-aed08689c8d4",
   "metadata": {},
   "source": [
    "## 2. Federated Learning with Flower\n",
    "\n",
    "Federated Learning (FL) is a technique that allows multiple devices or institutions to collaboratively train a model without exchanging raw data. Each participant trains the model locally and shares only model updates with a central server. This approach protects data privacy by keeping personal data decentralized. The following code uses the `flower` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a7f5a4-015f-4e38-bacd-5ddb13c55bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Server Code (run separately)\n",
    "import flwr as fl\n",
    "\n",
    "def fit_config(server_round):\n",
    "    return {\"epochs\": 1}\n",
    "\n",
    "fl.server.start_server(config={\"num_rounds\": 3}, strategy=fl.server.strategy.FedAvg(fit_config=fit_config))\n",
    "\n",
    "\n",
    "### Client Code\n",
    "\n",
    "\n",
    "import flwr as fl\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "model = nn.Sequential(nn.Flatten(), nn.Linear(28*28, 10))\n",
    "\n",
    "def get_data():\n",
    "    train = datasets.MNIST(\".\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "    return torch.utils.data.DataLoader(train, batch_size=32, shuffle=True)\n",
    "\n",
    "class FlowerClient(fl.client.NumPyClient):\n",
    "    def get_parameters(self): return [p.detach().numpy() for p in model.parameters()]\n",
    "    def fit(self, parameters, config):\n",
    "        for p, new_p in zip(model.parameters(), parameters):\n",
    "            p.data = torch.tensor(new_p)\n",
    "        loader = get_data()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "        for x, y in loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = nn.CrossEntropyLoss()(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        return self.get_parameters(), len(loader.dataset), {}\n",
    "    def evaluate(self, parameters, config): return 0.0, len(get_data().dataset), {}\n",
    "\n",
    "fl.client.start_numpy_client(server_address=\"localhost:8080\", client=FlowerClient())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5f902f-a190-4b3a-9d21-3f5ff07310ed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. PATE (Private Aggregation of Teacher Ensembles)\n",
    "\n",
    "PATE is a privacy technique that trains multiple teacher models on disjoint subsets of private data and uses their noisy aggregated outputs to train a student model. By adding noise to the aggregated predictions, PATE ensures strong privacy guarantees while enabling the student model to learn from the private data indirectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c9c582f-4b9d-4f8f-9ee4-7dfacec50d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "teacher_data = np.array_split(X_train, 5)\n",
    "teacher_labels = np.array_split(y_train, 5)\n",
    "\n",
    "teachers = [RandomForestClassifier().fit(x, y) for x, y in zip(teacher_data, teacher_labels)]\n",
    "\n",
    "def noisy_vote(x, epsilon=1.0):\n",
    "    votes = np.array([t.predict([x])[0] for t in teachers])\n",
    "    counts = np.bincount(votes, minlength=10)\n",
    "    noisy_counts = counts + np.random.laplace(0, 1/epsilon, size=10)\n",
    "    return np.argmax(noisy_counts)\n",
    "\n",
    "student_data = X_test\n",
    "student_labels = np.array([noisy_vote(x) for x in student_data])\n",
    "\n",
    "student = RandomForestClassifier().fit(student_data, student_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f989f5-ea45-4a7f-bd48-6888aec44b46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Synthetic Sample Filtering (Post-hoc Privacy Check)\n",
    "\n",
    "This method is a post-processing step used after generating synthetic data. It compares each synthetic sample to real data to check for overfitting or memorization risks. If a synthetic sample is too similar to a real one (based on a thresholded similarity metric like cosine similarity), it can be filtered out to preserve privacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8ab826d0-8590-446f-be1c-af073479a0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too similar?\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def is_too_similar(real_data, synthetic_sample, threshold=0.95):\n",
    "    sims = cosine_similarity([synthetic_sample], real_data)[0]\n",
    "    return np.any(sims > threshold)\n",
    "\n",
    "real = np.random.rand(100, 10)\n",
    "synth = np.random.rand(10)\n",
    "\n",
    "print(\"Too similar?\" if is_too_similar(real, synth) else \"Safe to keep\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046d8496-6e2f-4176-a432-addba66e4b5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
